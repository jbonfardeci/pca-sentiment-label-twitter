{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Label Prediction with PCA and Logistic Regression\n",
    "John Bonfardeci 2020-04-13\n",
    "\n",
    "This tutorial demonstrates how to use the Sci-kit learn Hash Vectorizer to hash text \n",
    "and use Principal Component Analysis for dimensionality reduction for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = pd.read_csv('stopwords.csv').values\n",
    "\n",
    "def get_cm(actual, predicted):\n",
    "    df = pd.DataFrame()\n",
    "    df['Actual'] = actual\n",
    "    df['Yhat'] = predicted\n",
    "    vals = df.values\n",
    "    dat = pd.DataFrame(columns=['TP', 'TN', 'FP', 'FN'], data=[])\n",
    "    dat['TP'] = list(map(lambda y: 1 if y[0] == y[1] and y[0] == True else 0, vals))\n",
    "    dat['TN'] = list(map(lambda y: 1 if y[0] == y[1] and y[0] == False else 0, vals))\n",
    "    dat['FP'] = list(map(lambda y: 1 if y[0] != y[1] and y[0] == False else 0, vals))\n",
    "    dat['FN'] = list(map(lambda y: 1 if y[0] != y[1] and y[0] == True else 0, vals))\n",
    "    tp = sum(dat['TP'])\n",
    "    tn = sum(dat['TN'])\n",
    "    fp = sum(dat['FP'])\n",
    "    fn = sum(dat['FN'])\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    stats = pd.DataFrame(columns=['TP', 'FP', 'TN', 'FN', 'Sensitivity', 'Specificity'],\n",
    "                data=[[tp, fp, tn, fn, tpr, tnr]])\n",
    "    return stats\n",
    "\n",
    "def get_ba(dat):\n",
    "    cm = get_cm(dat).values.tolist()[0]\n",
    "    sens = float(cm[4])\n",
    "    spec = float(cm[5])\n",
    "    return (sens+spec)/2\n",
    "\n",
    "def get_roc(actual, predicted, title=\"ROC\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve based on actuals and estimated probabilities.\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(actual, predicted)    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title(title)\n",
    "    lbl = 'AUC: {0:.3f}'.format(roc_auc)\n",
    "    plt.plot(fpr, tpr, 'b', label = lbl)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()  \n",
    "        \n",
    "def clean(s):\n",
    "        s = s.lower()\n",
    "        s = re.sub(\"^\\@\\w+\\s+\", \"\", s) # remove tweeter name (e.g. \"@elephantbird \")\n",
    "        s = re.sub(\"(^\\s+|\\s+$)\", \"\", s) # trim leading/trailing spaces\n",
    "        s = re.sub(\"[^a-z]\", \" \", s) # remove all but letters and spaces\n",
    "        w = [w for w in s.split(' ') if w not in stop_words]         \n",
    "        return ' '.join(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text\n",
       "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
       "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
       "2                0  @greeniebach I reckon he'll play, even if he's...\n",
       "3                0              @vaLewee I know!  Saw it on the news!\n",
       "4                0  very sad that http://www.fabchannel.com/ has c..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'twitter_dataset.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text\n",
    "Remove usernames, punctuation, convert lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>hey dear  happy friday  rice s bowl lunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ughhh layin downnnn    waiting zeina cook brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>reckon ll play  s        know      won t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>know   saw news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sad http   www fabchannel com  closed  web ser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text\n",
       "0                4        hey dear  happy friday  rice s bowl lunch  \n",
       "1                4  ughhh layin downnnn    waiting zeina cook brea...\n",
       "2                0          reckon ll play  s        know      won t \n",
       "3                0                                   know   saw news \n",
       "4                0  sad http   www fabchannel com  closed  web ser..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "col_name = \"tweet_text\"\n",
    "target_col = \"sentiment_label\"\n",
    "# Replace punctuation, special characters and digits with space\n",
    "sentiment = df[col_name].values\n",
    "df[col_name] = list( map(clean, sentiment) )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = HashingVectorizer(ngram_range=(1, 2), n_features=2**10)\n",
    "hash_vector = hasher.transform( df[col_name].values ).toarray()\n",
    "hash_vector[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00690226,  0.00662958,  0.00642173,  0.00605664,  0.00557674,\n",
       "        0.00546174,  0.00519327,  0.00490743,  0.00480731,  0.00463056])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10, random_state=123)\n",
    "pc = pca.fit_transform(hash_vector)\n",
    "pca.explained_variance_ratio_\n",
    "#pc[:10] # show first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into 70/30 Train/Validation sets\n",
    "y = list( map(lambda x: 0 if x == 0 else 1, df[target_col].values) )\n",
    "X_train, X_test, y_train, y_test = train_test_split(pc, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=123)\n",
    "logit.fit(X_train, y_train)\n",
    "predicted = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8372</td>\n",
       "      <td>4692</td>\n",
       "      <td>19214</td>\n",
       "      <td>15722</td>\n",
       "      <td>0.347472</td>\n",
       "      <td>0.803731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TP    FP     TN     FN  Sensitivity  Specificity\n",
       "0  8372  4692  19214  15722     0.347472     0.803731"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = get_cm(y_test, predicted)\n",
    "stats.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
